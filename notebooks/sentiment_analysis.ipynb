{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Benchmark with FinancialPhraseBank\n",
    "\n",
    "This notebook evaluates sentiment classification accuracy on the FinancialPhraseBank dataset using three tools:\n",
    "- NLTK VADER (lexicon-based)\n",
    "- FinBERT (finance-domain transformer)\n",
    "- DistilBERT (general-domain transformer)\n",
    "\n",
    "Data files are loaded from `data/external/FinancialPhraseBank-v1.0`. We report accuracy for multiple agreement splits (AllAgree, 75%, 66%, 50%). In our runs, FinBERT achieved the highest accuracy; VADER was moderate; DistilBERT underperformed due to its binary label space and limited handling of \"neutral\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating models on Sentences_AllAgree.txt ---\n",
      "VADER Accuracy: 0.5707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinBERT Accuracy: 0.9717\n",
      "DistilBERT Accuracy: 0.2584 (binary model, neutral handling may differ)\n",
      "\n",
      "--- Evaluating models on Sentences_75Agree.txt ---\n",
      "VADER Accuracy: 0.5627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinBERT Accuracy: 0.9473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT Accuracy: 0.2667 (binary model, neutral handling may differ)\n",
      "\n",
      "--- Evaluating models on Sentences_66Agree.txt ---\n",
      "VADER Accuracy: 0.5563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinBERT Accuracy: 0.9182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT Accuracy: 0.2912 (binary model, neutral handling may differ)\n",
      "\n",
      "--- Evaluating models on Sentences_50Agree.txt ---\n",
      "VADER Accuracy: 0.5429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinBERT Accuracy: 0.8896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT Accuracy: 0.2992 (binary model, neutral handling may differ)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Sentences_AllAgree.txt': {'vader': 0.5706713780918727,\n",
       "  'finbert': 0.9717314487632509,\n",
       "  'distilbert': 0.2583922261484099},\n",
       " 'Sentences_75Agree.txt': {'vader': 0.562699102229945,\n",
       "  'finbert': 0.9472922096727483,\n",
       "  'distilbert': 0.26672458731537796},\n",
       " 'Sentences_66Agree.txt': {'vader': 0.5563196585250177,\n",
       "  'finbert': 0.9181882855110268,\n",
       "  'distilbert': 0.29120227649988145},\n",
       " 'Sentences_50Agree.txt': {'vader': 0.5429219975237309,\n",
       "  'finbert': 0.8895996698307883,\n",
       "  'distilbert': 0.2992158481221626}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delegate to models/sentiment_analysis.py\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Resolve project root assuming this notebook runs from the `notebooks/` dir\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from models.sentiment_analysis import run_sentiment_benchmark\n",
    "\n",
    "DATASET_DIR = PROJECT_ROOT / 'data' / 'external' / 'FinancialPhraseBank-v1.0'\n",
    "\n",
    "metrics = run_sentiment_benchmark(\n",
    "    dataset_dir=str(DATASET_DIR),\n",
    "    focus_filename='Sentences_AllAgree.txt',\n",
    "    extra_filenames=('Sentences_75Agree.txt', 'Sentences_66Agree.txt', 'Sentences_50Agree.txt'),\n",
    "    run_vader=True,\n",
    "    run_finbert=True,\n",
    "    run_distilbert=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_53de9_row0_col1, #T_53de9_row1_col1, #T_53de9_row2_col1, #T_53de9_row3_col1 {\n",
       "  background-color: #d1fadf;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_53de9\">\n",
       "  <caption>Sentiment Accuracy by Dataset (higher is better)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_53de9_level0_col0\" class=\"col_heading level0 col0\" >vader</th>\n",
       "      <th id=\"T_53de9_level0_col1\" class=\"col_heading level0 col1\" >finbert</th>\n",
       "      <th id=\"T_53de9_level0_col2\" class=\"col_heading level0 col2\" >distilbert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_53de9_level0_row0\" class=\"row_heading level0 row0\" >Sentences_AllAgree.txt</th>\n",
       "      <td id=\"T_53de9_row0_col0\" class=\"data row0 col0\" >0.5707</td>\n",
       "      <td id=\"T_53de9_row0_col1\" class=\"data row0 col1\" >0.9717</td>\n",
       "      <td id=\"T_53de9_row0_col2\" class=\"data row0 col2\" >0.2584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_53de9_level0_row1\" class=\"row_heading level0 row1\" >Sentences_75Agree.txt</th>\n",
       "      <td id=\"T_53de9_row1_col0\" class=\"data row1 col0\" >0.5627</td>\n",
       "      <td id=\"T_53de9_row1_col1\" class=\"data row1 col1\" >0.9473</td>\n",
       "      <td id=\"T_53de9_row1_col2\" class=\"data row1 col2\" >0.2667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_53de9_level0_row2\" class=\"row_heading level0 row2\" >Sentences_66Agree.txt</th>\n",
       "      <td id=\"T_53de9_row2_col0\" class=\"data row2 col0\" >0.5563</td>\n",
       "      <td id=\"T_53de9_row2_col1\" class=\"data row2 col1\" >0.9182</td>\n",
       "      <td id=\"T_53de9_row2_col2\" class=\"data row2 col2\" >0.2912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_53de9_level0_row3\" class=\"row_heading level0 row3\" >Sentences_50Agree.txt</th>\n",
       "      <td id=\"T_53de9_row3_col0\" class=\"data row3 col0\" >0.5429</td>\n",
       "      <td id=\"T_53de9_row3_col1\" class=\"data row3 col1\" >0.8896</td>\n",
       "      <td id=\"T_53de9_row3_col2\" class=\"data row3 col2\" >0.2992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x106442cf0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_model</th>\n",
       "      <th>best_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sentences_50Agree.txt</th>\n",
       "      <td>finbert</td>\n",
       "      <td>0.8896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentences_66Agree.txt</th>\n",
       "      <td>finbert</td>\n",
       "      <td>0.9182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentences_75Agree.txt</th>\n",
       "      <td>finbert</td>\n",
       "      <td>0.9473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentences_AllAgree.txt</th>\n",
       "      <td>finbert</td>\n",
       "      <td>0.9717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       best_model  best_accuracy\n",
       "Sentences_50Agree.txt     finbert         0.8896\n",
       "Sentences_66Agree.txt     finbert         0.9182\n",
       "Sentences_75Agree.txt     finbert         0.9473\n",
       "Sentences_AllAgree.txt    finbert         0.9717"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a clean summary table from `metrics` and save to results\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# `metrics` is a dict like {filename: {model: accuracy}}\n",
    "df = pd.DataFrame(metrics).T\n",
    "\n",
    "# Ensure consistent column order\n",
    "model_cols = ['vader', 'finbert', 'distilbert']\n",
    "for c in model_cols:\n",
    "    if c not in df.columns:\n",
    "        df[c] = pd.NA\n",
    "\n",
    "# Reorder and cast to float where possible\n",
    "df = df[model_cols]\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Round for display and compute best model per dataset\n",
    "summary = df.round(4)\n",
    "summary['best_model'] = summary[model_cols].idxmax(axis=1)\n",
    "summary['best_accuracy'] = summary[model_cols].max(axis=1)\n",
    "\n",
    "# Save to CSV under results\n",
    "results_path = Path(PROJECT_ROOT) / 'results' / 'sentiment_benchmark_metrics.csv'\n",
    "results_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "summary.to_csv(results_path, index_label='dataset')\n",
    "\n",
    "# Display nicely formatted table with row-wise max highlighted\n",
    "styled = (\n",
    "    summary[model_cols]\n",
    "        .style\n",
    "        .format('{:.4f}')\n",
    "        .highlight_max(axis=1, color='#d1fadf')\n",
    "        .set_caption('Sentiment Accuracy by Dataset (higher is better)')\n",
    ")\n",
    "\n",
    "display(styled)\n",
    "\n",
    "# Also display the chosen best model per dataset\n",
    "summary[['best_model', 'best_accuracy']].sort_index()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
