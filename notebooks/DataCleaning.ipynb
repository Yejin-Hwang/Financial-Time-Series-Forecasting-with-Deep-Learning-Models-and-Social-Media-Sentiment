{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e484948d",
   "metadata": {},
   "source": [
    "### Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7207adcd",
   "metadata": {},
   "source": [
    "### TSLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da631564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from typing import Iterable\n",
    "\n",
    "# Resolve project root (works from repo root or notebooks/)\n",
    "cwd = Path.cwd().resolve()\n",
    "PROJECT_ROOT = cwd.parent if cwd.name == 'notebooks' else cwd\n",
    "\n",
    "price_path = PROJECT_ROOT / 'data' / 'interim' / 'NVDA_price_full.csv'\n",
    "sent_path  = PROJECT_ROOT / 'data' / 'interim' / 'sentiment_scoring' / 'nvda_daily_sentiment_score.csv'\n",
    "spike_path = PROJECT_ROOT / 'data' / 'interim' / 'activitiy_recognition' / 'nvda_spike_data.csv'  # folder name as-is\n",
    "\n",
    "out_path = PROJECT_ROOT / 'data' / 'processed' / 'nvda_price_sentiment_spike_merged.csv'\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def coerce_date_col(df: pd.DataFrame, candidates: Iterable[str]) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # candidate list + heuristics (any column containing date/time hints)\n",
    "    heuristics = [c for c in df.columns if any(k in c.lower() for k in ('date','day','time','timestamp','created'))]\n",
    "    for col in list(dict.fromkeys([*candidates, *heuristics])):  # preserve order, dedupe\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        s = df[col]\n",
    "        dt = pd.to_datetime(s, utc=True, errors='coerce')  # strings or datetimes\n",
    "        # try epoch seconds/millis if needed\n",
    "        if dt.isna().all() and pd.api.types.is_numeric_dtype(s):\n",
    "            dt = pd.to_datetime(s, unit='s', utc=True, errors='coerce')\n",
    "            if dt.isna().all():\n",
    "                dt = pd.to_datetime(s, unit='ms', utc=True, errors='coerce')\n",
    "        if dt.isna().all():\n",
    "            continue\n",
    "        # strip tz and keep date\n",
    "        try:\n",
    "            dt = dt.dt.tz_localize(None)\n",
    "        except Exception:\n",
    "            pass\n",
    "        df['date'] = dt.dt.floor('D')\n",
    "        return df\n",
    "    raise ValueError(f'No date-like column found. columns={list(df.columns)}')\n",
    "\n",
    "# 1) Price (anchor; keep all dates)\n",
    "price = pd.read_csv(price_path)\n",
    "price = coerce_date_col(price, candidates=('Date','date'))\n",
    "price = price.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# 2) Sentiment (daily)\n",
    "sent = pd.read_csv(sent_path)\n",
    "sent = coerce_date_col(sent, candidates=('date','Date'))\n",
    "# normalize sentiment column name\n",
    "if 'daily_sentiment' not in sent.columns:\n",
    "    score_cols = [c for c in sent.columns if 'sentiment' in c.lower()]\n",
    "    if not score_cols:\n",
    "        raise ValueError(f'No sentiment score column found in {sent_path}')\n",
    "    sent = sent.rename(columns={score_cols[0]: 'daily_sentiment'})\n",
    "sent = sent[['date','daily_sentiment']].drop_duplicates('date')\n",
    "\n",
    "# 3) Spike/activity\n",
    "spike = pd.read_csv(spike_path)\n",
    "spike = coerce_date_col(\n",
    "    spike,\n",
    "    candidates=('date','Date','datetime','impact_day','impact_date','impact_trading_day','created_utc','created_at','timestamp')\n",
    ")\n",
    "\n",
    "# keep only known/likely spike columns if present\n",
    "spike_candidates = ['post_count','spike_presence','spike_intensity']\n",
    "present_spike_cols = [c for c in spike_candidates if c in spike.columns]\n",
    "spike = spike[['date', *present_spike_cols]].copy()\n",
    "\n",
    "# if multiple rows per date exist, aggregate reasonably\n",
    "if spike.duplicated('date').any():\n",
    "    agg = {}\n",
    "    for c in present_spike_cols:\n",
    "        if 'count' in c:\n",
    "            agg[c] = 'sum'\n",
    "        elif 'presence' in c:\n",
    "            agg[c] = 'sum'  # if presence is daily count/flag; adjust to 'max' if you prefer\n",
    "        elif 'intensity' in c:\n",
    "            agg[c] = 'max'\n",
    "        else:\n",
    "            agg[c] = 'first'\n",
    "    spike = spike.groupby('date', as_index=False).agg(agg)\n",
    "\n",
    "# 4) Merge (LEFT on price)\n",
    "merged = price.merge(sent, on='date', how='left').merge(spike, on='date', how='left')\n",
    "\n",
    "# Save\n",
    "merged.to_csv(out_path, index=False)\n",
    "print('rows:', len(merged))\n",
    "print('saved:', out_path)\n",
    "print('columns:', merged.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e1f73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged\n",
    "\n",
    "\n",
    "#select date from 2025-02-03 to 2025-10-16\n",
    "df= df[df['date'] >= '2025-02-03']\n",
    "df = df[df['date'] <= '2025-07-17']\n",
    "\n",
    "\n",
    "#how many daily sentiment are NaN?\n",
    "print(df['daily_sentiment'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60617ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment\n",
    "df['daily_sentiment'] = pd.to_numeric(df['daily_sentiment'], errors='coerce')\n",
    "df['has_sentiment'] = df['daily_sentiment'].notna().astype(int)\n",
    "s = df['daily_sentiment'].shift(1)\n",
    "for w in (3,7,14):\n",
    "    df[f'daily_sentiment_mean_{w}'] = s.rolling(w, min_periods=1).mean()\n",
    "    df[f'daily_sentiment_std_{w}']  = s.rolling(w, min_periods=1).std()\n",
    "\n",
    "# spike\n",
    "for c in ('post_count','spike_presence','spike_intensity'):\n",
    "    if c in df:\n",
    "        df[c] = df[c].fillna(0)\n",
    "df['has_spike'] = ((df.get('post_count',0)>0) | (df.get('spike_presence',0)>0) | (df.get('spike_intensity',0)>0)).astype(int)\n",
    "\n",
    "# 입력 컬럼 NaN 제거(롤링/래그 등만)\n",
    "input_cols = [c for c in df.columns if c.startswith('daily_sentiment_')] + \\\n",
    "             [c for c in ('post_count','spike_presence','spike_intensity') if c in df]\n",
    "df[input_cols] = df[input_cols].fillna(method='ffill').fillna(0)\n",
    "\n",
    "\n",
    "#drop open,high,low columns\n",
    "# df = df.drop(columns=['Open', 'High', 'Low'])\n",
    "\n",
    "# Standardize column names\n",
    "df = df.rename(columns={\"Close\":\"close\",\"Volume\":\"volume\"})\n",
    "if \"Date\" in df.columns: df = df.drop(columns=[\"Date\"])\n",
    "\n",
    "# Sort and index\n",
    "df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "if \"time_idx\" not in df.columns:\n",
    "    df[\"time_idx\"] = range(len(df))\n",
    "if \"unique_id\" not in df.columns:\n",
    "    df[\"unique_id\"] = \"NVDA\"\n",
    "\n",
    "# Fill missing spike columns with 0 (meaning absence)\n",
    "for c in (\"post_count\",\"spike_presence\",\"spike_intensity\"):\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].fillna(0)\n",
    "\n",
    "# Sentiment rolling features should not contain NaN (allow only ffill/bfill right before model input)\n",
    "sent_cols = [c for c in df.columns if c.startswith(\"daily_sentiment_\")]\n",
    "if sent_cols:\n",
    "    df[sent_cols] = df[sent_cols].ffill().bfill()\n",
    "\n",
    "df_to_csv = df.to_csv('data/processed/nvda_price_sentiment_spike_merged_20250203_20250717.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c94c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d013a9ac",
   "metadata": {},
   "source": [
    "### NVDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "85868db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "PROJECT_ROOT = Path('/Users/hwang-yejin/Desktop/Financial Time Series Forecasting with Deep Learning Models and Social Media Sentiment')\n",
    "\n",
    "PRICE_CSV = PROJECT_ROOT / 'data' / 'processed' / 'TSLA_price_full.csv'\n",
    "SPIKE_CSV = PROJECT_ROOT / 'data' / 'interim' / 'activitiy_recognition' / 'tsla_spike_data.csv'\n",
    "SENTI_CSV = PROJECT_ROOT / 'data' / 'interim' / 'sentiment_scoring' / 'tsla_daily_sentiment_score.csv'\n",
    "OUT_DIR   = PROJECT_ROOT / 'data' / 'processed'\n",
    "\n",
    "def coerce_date_col(df: pd.DataFrame, candidates=('date','Date','datetime','timestamp','created','time','day')) -> pd.DataFrame:\n",
    "  df = df.copy()\n",
    "  # try explicit candidates first, then heuristic scan\n",
    "  heuristics = [c for c in df.columns if any(k in c.lower() for k in ('date','day','time','timestamp','created'))]\n",
    "  for col in list(candidates) + heuristics:\n",
    "    if col not in df.columns:\n",
    "      continue\n",
    "    s = df[col]\n",
    "    # try generic parse\n",
    "    dt = pd.to_datetime(s, utc=True, errors='coerce')\n",
    "    # try seconds epoch\n",
    "    if dt.isna().all() and pd.api.types.is_numeric_dtype(s):\n",
    "      dt = pd.to_datetime(s, unit='s', utc=True, errors='coerce')\n",
    "      # try milliseconds epoch\n",
    "      if dt.isna().all():\n",
    "        dt = pd.to_datetime(s, unit='ms', utc=True, errors='coerce')\n",
    "    if not dt.isna().all():\n",
    "      df['date'] = dt.dt.tz_localize(None).dt.floor('D')\n",
    "      return df\n",
    "  raise ValueError('No date-like column found')\n",
    "\n",
    "def group_by_date_mean_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
    "  if 'date' not in df.columns:\n",
    "    return df\n",
    "  if not df['date'].is_monotonic_increasing or df.duplicated('date').any():\n",
    "    num_cols = df.select_dtypes(include=[np.number, 'boolean']).columns.tolist()\n",
    "    num_cols = [c for c in num_cols if c != 'date']\n",
    "    agg = {c: 'mean' for c in num_cols}\n",
    "    # for non-numeric, keep first\n",
    "    for c in df.columns:\n",
    "      if c not in agg and c != 'date':\n",
    "        agg[c] = 'first'\n",
    "    df = (df.groupby('date', as_index=False).agg(agg))\n",
    "  return df\n",
    "\n",
    "# 1) Load price and normalize date\n",
    "price = pd.read_csv(PRICE_CSV)\n",
    "# TSLA_price_full already has 'Date'; keep everything and add 'date'\n",
    "price_date = pd.to_datetime(price['Date'], utc=True, errors='coerce').dt.tz_localize(None).dt.floor('D')\n",
    "price = price.assign(date=price_date).sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# 2) Load spike, coerce date, collapse duplicates\n",
    "spike = pd.read_csv(SPIKE_CSV)\n",
    "spike = coerce_date_col(spike)\n",
    "spike = group_by_date_mean_numeric(spike)\n",
    "\n",
    "# 3) Load sentiment, coerce date, collapse duplicates\n",
    "senti = pd.read_csv(SENTI_CSV)\n",
    "senti = coerce_date_col(senti)\n",
    "senti = group_by_date_mean_numeric(senti)\n",
    "\n",
    "# Optional: add prefixes to avoid accidental name collisions (skip 'date')\n",
    "def add_prefix(df, prefix):\n",
    "  keep = ['date']\n",
    "  cols = {c: (c if c in keep else f'{prefix}{c}') for c in df.columns}\n",
    "  return df.rename(columns=cols)\n",
    "\n",
    "spike = add_prefix(spike, 'spike_')\n",
    "\n",
    "# 4) Left-join on price dates (anchor)\n",
    "merged = (\n",
    "  price.merge(senti, on='date', how='left', suffixes=('', '_dup'))\n",
    "       .merge(spike, on='date', how='left', suffixes=('', '_dup2'))\n",
    "       .sort_values('date')\n",
    "       .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Drop any accidental duplicate helper columns if created\n",
    "dup_cols = [c for c in merged.columns if c.endswith('_dup') or c.endswith('_dup2')]\n",
    "if dup_cols:\n",
    "  merged = merged.drop(columns=dup_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c7ba12cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= merged \n",
    "\n",
    "#select date from 2025-02-03 to 2025-10-16\n",
    "df= df[df['date'] >= '2025-02-03']\n",
    "df = df[df['date'] <= '2025-07-17']\n",
    "\n",
    "df.rename(columns={'daily_sentiment_daily_sentiment':'daily_sentiment'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3bd6da73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(columns=['Open', 'High', 'Low'], inplace=True)\n",
    "# df.rename(columns = {\"Close\":\"close\",\"Volume\":\"volume\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8cdcd364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>last_earnings_date</th>\n",
       "      <th>days_since_earning</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>date</th>\n",
       "      <th>daily_sentiment</th>\n",
       "      <th>spike_impact_trading_day</th>\n",
       "      <th>spike_post_count</th>\n",
       "      <th>spike_smoothed</th>\n",
       "      <th>spike_spike_presence</th>\n",
       "      <th>spike_spike_intensity</th>\n",
       "      <th>spike_loess_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>2025-02-03</td>\n",
       "      <td>386.679993</td>\n",
       "      <td>389.170013</td>\n",
       "      <td>374.359985</td>\n",
       "      <td>383.679993</td>\n",
       "      <td>93732100</td>\n",
       "      <td>2025-01-29 16:12:00-05:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>636</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>2025-02-03</td>\n",
       "      <td>0.846911</td>\n",
       "      <td>2025-02-03</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.551097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.519992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>2025-02-04</td>\n",
       "      <td>382.630005</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>381.399994</td>\n",
       "      <td>392.209991</td>\n",
       "      <td>57072200</td>\n",
       "      <td>2025-01-29 16:12:00-05:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>637</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>2025-02-04</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>2025-02-04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.248485</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.217380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>2025-02-05</td>\n",
       "      <td>387.510010</td>\n",
       "      <td>388.390015</td>\n",
       "      <td>375.529999</td>\n",
       "      <td>378.170013</td>\n",
       "      <td>57223300</td>\n",
       "      <td>2025-01-29 16:12:00-05:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>638</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>2025-02-05</td>\n",
       "      <td>0.868623</td>\n",
       "      <td>2025-02-05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.983199</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.952094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>2025-02-06</td>\n",
       "      <td>373.029999</td>\n",
       "      <td>375.399994</td>\n",
       "      <td>363.179993</td>\n",
       "      <td>374.320007</td>\n",
       "      <td>77918200</td>\n",
       "      <td>2025-01-29 16:12:00-05:00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>639</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>2025-02-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2025-02-06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.852118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.821012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>2025-02-07</td>\n",
       "      <td>370.190002</td>\n",
       "      <td>380.549988</td>\n",
       "      <td>360.339996</td>\n",
       "      <td>361.619995</td>\n",
       "      <td>70298300</td>\n",
       "      <td>2025-01-29 16:12:00-05:00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>640</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>2025-02-07</td>\n",
       "      <td>0.887983</td>\n",
       "      <td>2025-02-07</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.876135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.845030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>2025-07-11</td>\n",
       "      <td>307.890015</td>\n",
       "      <td>314.089996</td>\n",
       "      <td>305.649994</td>\n",
       "      <td>313.510010</td>\n",
       "      <td>79236400</td>\n",
       "      <td>2025-04-22 16:07:00-04:00</td>\n",
       "      <td>79.0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>745</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>2025-07-11</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2025-07-11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.860793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.829688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>2025-07-14</td>\n",
       "      <td>317.730011</td>\n",
       "      <td>322.600006</td>\n",
       "      <td>312.670013</td>\n",
       "      <td>316.899994</td>\n",
       "      <td>78043400</td>\n",
       "      <td>2025-04-22 16:07:00-04:00</td>\n",
       "      <td>82.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>746</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>2025-07-14</td>\n",
       "      <td>0.889710</td>\n",
       "      <td>2025-07-14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.770486</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.739381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>2025-07-15</td>\n",
       "      <td>319.679993</td>\n",
       "      <td>321.200012</td>\n",
       "      <td>310.500000</td>\n",
       "      <td>310.779999</td>\n",
       "      <td>77556300</td>\n",
       "      <td>2025-04-22 16:07:00-04:00</td>\n",
       "      <td>83.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>747</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>2025-07-15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-07-15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.259119</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.228013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>2025-07-16</td>\n",
       "      <td>312.799988</td>\n",
       "      <td>323.500000</td>\n",
       "      <td>312.619995</td>\n",
       "      <td>321.670013</td>\n",
       "      <td>97284800</td>\n",
       "      <td>2025-04-22 16:07:00-04:00</td>\n",
       "      <td>84.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>748</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>2025-07-16</td>\n",
       "      <td>0.891185</td>\n",
       "      <td>2025-07-16</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.876913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.845808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>2025-07-17</td>\n",
       "      <td>323.149994</td>\n",
       "      <td>324.339996</td>\n",
       "      <td>317.059998</td>\n",
       "      <td>319.410004</td>\n",
       "      <td>73922900</td>\n",
       "      <td>2025-04-22 16:07:00-04:00</td>\n",
       "      <td>85.0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>749</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>2025-07-17</td>\n",
       "      <td>0.387638</td>\n",
       "      <td>2025-07-17</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.558537</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.527432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Open        High         Low       Close    Volume  \\\n",
       "636  2025-02-03  386.679993  389.170013  374.359985  383.679993  93732100   \n",
       "637  2025-02-04  382.630005  394.000000  381.399994  392.209991  57072200   \n",
       "638  2025-02-05  387.510010  388.390015  375.529999  378.170013  57223300   \n",
       "639  2025-02-06  373.029999  375.399994  363.179993  374.320007  77918200   \n",
       "640  2025-02-07  370.190002  380.549988  360.339996  361.619995  70298300   \n",
       "..          ...         ...         ...         ...         ...       ...   \n",
       "745  2025-07-11  307.890015  314.089996  305.649994  313.510010  79236400   \n",
       "746  2025-07-14  317.730011  322.600006  312.670013  316.899994  78043400   \n",
       "747  2025-07-15  319.679993  321.200012  310.500000  310.779999  77556300   \n",
       "748  2025-07-16  312.799988  323.500000  312.619995  321.670013  97284800   \n",
       "749  2025-07-17  323.149994  324.339996  317.059998  319.410004  73922900   \n",
       "\n",
       "            last_earnings_date  days_since_earning  month  day_of_week  ...  \\\n",
       "636  2025-01-29 16:12:00-05:00                 4.0      2            0  ...   \n",
       "637  2025-01-29 16:12:00-05:00                 5.0      2            1  ...   \n",
       "638  2025-01-29 16:12:00-05:00                 6.0      2            2  ...   \n",
       "639  2025-01-29 16:12:00-05:00                 7.0      2            3  ...   \n",
       "640  2025-01-29 16:12:00-05:00                 8.0      2            4  ...   \n",
       "..                         ...                 ...    ...          ...  ...   \n",
       "745  2025-04-22 16:07:00-04:00                79.0      7            4  ...   \n",
       "746  2025-04-22 16:07:00-04:00                82.0      7            0  ...   \n",
       "747  2025-04-22 16:07:00-04:00                83.0      7            1  ...   \n",
       "748  2025-04-22 16:07:00-04:00                84.0      7            2  ...   \n",
       "749  2025-04-22 16:07:00-04:00                85.0      7            3  ...   \n",
       "\n",
       "     time_idx  unique_id       date  daily_sentiment  \\\n",
       "636       636       TSLA 2025-02-03         0.846911   \n",
       "637       637       TSLA 2025-02-04        -0.666667   \n",
       "638       638       TSLA 2025-02-05         0.868623   \n",
       "639       639       TSLA 2025-02-06         0.000000   \n",
       "640       640       TSLA 2025-02-07         0.887983   \n",
       "..        ...        ...        ...              ...   \n",
       "745       745       TSLA 2025-07-11         0.500000   \n",
       "746       746       TSLA 2025-07-14         0.889710   \n",
       "747       747       TSLA 2025-07-15              NaN   \n",
       "748       748       TSLA 2025-07-16         0.891185   \n",
       "749       749       TSLA 2025-07-17         0.387638   \n",
       "\n",
       "     spike_impact_trading_day  spike_post_count  spike_smoothed  \\\n",
       "636                2025-02-03               4.0        2.551097   \n",
       "637                2025-02-04               1.0        2.248485   \n",
       "638                2025-02-05               1.0        1.983199   \n",
       "639                2025-02-06               1.0        1.852118   \n",
       "640                2025-02-07               2.0        1.876135   \n",
       "..                        ...               ...             ...   \n",
       "745                2025-07-11               1.0        1.860793   \n",
       "746                2025-07-14               1.0        1.770486   \n",
       "747                2025-07-15               1.0        2.259119   \n",
       "748                2025-07-16               2.0        2.876913   \n",
       "749                2025-07-17               4.0        3.558537   \n",
       "\n",
       "     spike_spike_presence spike_spike_intensity spike_loess_upper  \n",
       "636                   0.0                   0.0          5.519992  \n",
       "637                   0.0                   0.0          5.217380  \n",
       "638                   0.0                   0.0          4.952094  \n",
       "639                   0.0                   0.0          4.821012  \n",
       "640                   0.0                   0.0          4.845030  \n",
       "..                    ...                   ...               ...  \n",
       "745                   0.0                   0.0          4.829688  \n",
       "746                   0.0                   0.0          4.739381  \n",
       "747                   0.0                   0.0          5.228013  \n",
       "748                   0.0                   0.0          5.845808  \n",
       "749                   0.0                   0.0          6.527432  \n",
       "\n",
       "[114 rows x 27 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7910918c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "#how many daily sentiment are NaN?\n",
    "print(df['daily_sentiment'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0d6047b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment\n",
    "df['daily_sentiment'] = pd.to_numeric(df['daily_sentiment'], errors='coerce')\n",
    "\n",
    "df['has_sentiment'] = df['daily_sentiment'].notna().astype(int)\n",
    "df['has_sentiment'] = df['daily_sentiment'].notna().astype(int)\n",
    "\n",
    "\n",
    "s = df['daily_sentiment'].shift(1)\n",
    "for w in (3,7,14):\n",
    "    df[f'daily_sentiment_mean_{w}'] = s.rolling(w, min_periods=1).mean()\n",
    "    df[f'daily_sentiment_std_{w}']  = s.rolling(w, min_periods=1).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5655807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#spike\n",
    "for c in ('post_count', 'spike_presence', 'spike_intensity'):\n",
    "    if c in df:\n",
    "        df[c] = df[c].fillna(0)\n",
    "df['has_spike'] = (\n",
    "    (df.get('post_count', pd.Series(0, index=df.index)) > 0) |\n",
    "    (df.get('spike_presence', pd.Series(0, index=df.index)) > 0) |\n",
    "    (df.get('spike_intensity', pd.Series(0, index=df.index)) > 0)\n",
    ").astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# Sort and index\n",
    "df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "if \"time_idx\" not in df.columns:\n",
    "    df[\"time_idx\"] = range(len(df))\n",
    "if \"unique_id\" not in df.columns:\n",
    "    df[\"unique_id\"] = \"NVDA\"\n",
    "\n",
    "# Fill missing spike columns with 0 (meaning absence)\n",
    "for c in (\"post_count\",\"spike_presence\",\"spike_intensity\"):\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].fillna(0)\n",
    "\n",
    "# Sentiment rolling features should not contain NaN (allow only ffill/bfill right before model input)\n",
    "sent_cols = [c for c in df.columns if c.startswith(\"daily_sentiment_\")]\n",
    "if sent_cols:\n",
    "    df[sent_cols] = df[sent_cols].ffill().bfill()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "767bae58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pq/ywh3rrys6n9dnn8yfd90rznm0000gn/T/ipykernel_35998/4101972365.py:4: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[input_cols] = df[input_cols].fillna(method='ffill').fillna(0)\n"
     ]
    }
   ],
   "source": [
    "# Remove NaN values from input columns (only rolling/lag features)\n",
    "input_cols = [c for c in df.columns if c.startswith('daily_sentiment_')] + \\\n",
    "             [c for c in ('post_count','spike_presence','spike_intensity') if c in df]\n",
    "df[input_cols] = df[input_cols].fillna(method='ffill').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "de56b14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>last_earnings_date</th>\n",
       "      <th>days_since_earning</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>spike_spike_intensity</th>\n",
       "      <th>spike_loess_upper</th>\n",
       "      <th>has_sentiment</th>\n",
       "      <th>daily_sentiment_mean_3</th>\n",
       "      <th>daily_sentiment_std_3</th>\n",
       "      <th>daily_sentiment_mean_7</th>\n",
       "      <th>daily_sentiment_std_7</th>\n",
       "      <th>daily_sentiment_mean_14</th>\n",
       "      <th>daily_sentiment_std_14</th>\n",
       "      <th>has_spike</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-02-03</td>\n",
       "      <td>386.679993</td>\n",
       "      <td>389.170013</td>\n",
       "      <td>374.359985</td>\n",
       "      <td>383.679993</td>\n",
       "      <td>93732100</td>\n",
       "      <td>2025-01-29 16:12:00-05:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.519992</td>\n",
       "      <td>1</td>\n",
       "      <td>0.846911</td>\n",
       "      <td>1.070261</td>\n",
       "      <td>0.846911</td>\n",
       "      <td>1.070261</td>\n",
       "      <td>0.846911</td>\n",
       "      <td>1.070261</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-02-04</td>\n",
       "      <td>382.630005</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>381.399994</td>\n",
       "      <td>392.209991</td>\n",
       "      <td>57072200</td>\n",
       "      <td>2025-01-29 16:12:00-05:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.217380</td>\n",
       "      <td>1</td>\n",
       "      <td>0.846911</td>\n",
       "      <td>1.070261</td>\n",
       "      <td>0.846911</td>\n",
       "      <td>1.070261</td>\n",
       "      <td>0.846911</td>\n",
       "      <td>1.070261</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-02-05</td>\n",
       "      <td>387.510010</td>\n",
       "      <td>388.390015</td>\n",
       "      <td>375.529999</td>\n",
       "      <td>378.170013</td>\n",
       "      <td>57223300</td>\n",
       "      <td>2025-01-29 16:12:00-05:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.952094</td>\n",
       "      <td>1</td>\n",
       "      <td>0.090122</td>\n",
       "      <td>1.070261</td>\n",
       "      <td>0.090122</td>\n",
       "      <td>1.070261</td>\n",
       "      <td>0.090122</td>\n",
       "      <td>1.070261</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-02-06</td>\n",
       "      <td>373.029999</td>\n",
       "      <td>375.399994</td>\n",
       "      <td>363.179993</td>\n",
       "      <td>374.320007</td>\n",
       "      <td>77918200</td>\n",
       "      <td>2025-01-29 16:12:00-05:00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.821012</td>\n",
       "      <td>1</td>\n",
       "      <td>0.349622</td>\n",
       "      <td>0.880199</td>\n",
       "      <td>0.349622</td>\n",
       "      <td>0.880199</td>\n",
       "      <td>0.349622</td>\n",
       "      <td>0.880199</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-02-07</td>\n",
       "      <td>370.190002</td>\n",
       "      <td>380.549988</td>\n",
       "      <td>360.339996</td>\n",
       "      <td>361.619995</td>\n",
       "      <td>70298300</td>\n",
       "      <td>2025-01-29 16:12:00-05:00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.845030</td>\n",
       "      <td>1</td>\n",
       "      <td>0.067319</td>\n",
       "      <td>0.769856</td>\n",
       "      <td>0.262217</td>\n",
       "      <td>0.739635</td>\n",
       "      <td>0.262217</td>\n",
       "      <td>0.739635</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>2025-07-11</td>\n",
       "      <td>307.890015</td>\n",
       "      <td>314.089996</td>\n",
       "      <td>305.649994</td>\n",
       "      <td>313.510010</td>\n",
       "      <td>79236400</td>\n",
       "      <td>2025-04-22 16:07:00-04:00</td>\n",
       "      <td>79.0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.829688</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.033357</td>\n",
       "      <td>0.016115</td>\n",
       "      <td>0.368044</td>\n",
       "      <td>0.370627</td>\n",
       "      <td>0.468990</td>\n",
       "      <td>0.300142</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>2025-07-14</td>\n",
       "      <td>317.730011</td>\n",
       "      <td>322.600006</td>\n",
       "      <td>312.670013</td>\n",
       "      <td>316.899994</td>\n",
       "      <td>78043400</td>\n",
       "      <td>2025-04-22 16:07:00-04:00</td>\n",
       "      <td>82.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.739381</td>\n",
       "      <td>1</td>\n",
       "      <td>0.144428</td>\n",
       "      <td>0.308145</td>\n",
       "      <td>0.311235</td>\n",
       "      <td>0.303751</td>\n",
       "      <td>0.450591</td>\n",
       "      <td>0.289895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>2025-07-15</td>\n",
       "      <td>319.679993</td>\n",
       "      <td>321.200012</td>\n",
       "      <td>310.500000</td>\n",
       "      <td>310.779999</td>\n",
       "      <td>77556300</td>\n",
       "      <td>2025-04-22 16:07:00-04:00</td>\n",
       "      <td>83.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.228013</td>\n",
       "      <td>0</td>\n",
       "      <td>0.455916</td>\n",
       "      <td>0.457432</td>\n",
       "      <td>0.421193</td>\n",
       "      <td>0.378631</td>\n",
       "      <td>0.482944</td>\n",
       "      <td>0.316531</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>2025-07-16</td>\n",
       "      <td>312.799988</td>\n",
       "      <td>323.500000</td>\n",
       "      <td>312.619995</td>\n",
       "      <td>321.670013</td>\n",
       "      <td>97284800</td>\n",
       "      <td>2025-04-22 16:07:00-04:00</td>\n",
       "      <td>84.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.845808</td>\n",
       "      <td>1</td>\n",
       "      <td>0.694855</td>\n",
       "      <td>0.275567</td>\n",
       "      <td>0.400608</td>\n",
       "      <td>0.419552</td>\n",
       "      <td>0.481821</td>\n",
       "      <td>0.331955</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>2025-07-17</td>\n",
       "      <td>323.149994</td>\n",
       "      <td>324.339996</td>\n",
       "      <td>317.059998</td>\n",
       "      <td>319.410004</td>\n",
       "      <td>73922900</td>\n",
       "      <td>2025-04-22 16:07:00-04:00</td>\n",
       "      <td>85.0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.527432</td>\n",
       "      <td>1</td>\n",
       "      <td>0.890447</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.442836</td>\n",
       "      <td>0.463077</td>\n",
       "      <td>0.489599</td>\n",
       "      <td>0.341175</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Open        High         Low       Close    Volume  \\\n",
       "0    2025-02-03  386.679993  389.170013  374.359985  383.679993  93732100   \n",
       "1    2025-02-04  382.630005  394.000000  381.399994  392.209991  57072200   \n",
       "2    2025-02-05  387.510010  388.390015  375.529999  378.170013  57223300   \n",
       "3    2025-02-06  373.029999  375.399994  363.179993  374.320007  77918200   \n",
       "4    2025-02-07  370.190002  380.549988  360.339996  361.619995  70298300   \n",
       "..          ...         ...         ...         ...         ...       ...   \n",
       "109  2025-07-11  307.890015  314.089996  305.649994  313.510010  79236400   \n",
       "110  2025-07-14  317.730011  322.600006  312.670013  316.899994  78043400   \n",
       "111  2025-07-15  319.679993  321.200012  310.500000  310.779999  77556300   \n",
       "112  2025-07-16  312.799988  323.500000  312.619995  321.670013  97284800   \n",
       "113  2025-07-17  323.149994  324.339996  317.059998  319.410004  73922900   \n",
       "\n",
       "            last_earnings_date  days_since_earning  month  day_of_week  ...  \\\n",
       "0    2025-01-29 16:12:00-05:00                 4.0      2            0  ...   \n",
       "1    2025-01-29 16:12:00-05:00                 5.0      2            1  ...   \n",
       "2    2025-01-29 16:12:00-05:00                 6.0      2            2  ...   \n",
       "3    2025-01-29 16:12:00-05:00                 7.0      2            3  ...   \n",
       "4    2025-01-29 16:12:00-05:00                 8.0      2            4  ...   \n",
       "..                         ...                 ...    ...          ...  ...   \n",
       "109  2025-04-22 16:07:00-04:00                79.0      7            4  ...   \n",
       "110  2025-04-22 16:07:00-04:00                82.0      7            0  ...   \n",
       "111  2025-04-22 16:07:00-04:00                83.0      7            1  ...   \n",
       "112  2025-04-22 16:07:00-04:00                84.0      7            2  ...   \n",
       "113  2025-04-22 16:07:00-04:00                85.0      7            3  ...   \n",
       "\n",
       "     spike_spike_intensity  spike_loess_upper  has_sentiment  \\\n",
       "0                      0.0           5.519992              1   \n",
       "1                      0.0           5.217380              1   \n",
       "2                      0.0           4.952094              1   \n",
       "3                      0.0           4.821012              1   \n",
       "4                      0.0           4.845030              1   \n",
       "..                     ...                ...            ...   \n",
       "109                    0.0           4.829688              1   \n",
       "110                    0.0           4.739381              1   \n",
       "111                    0.0           5.228013              0   \n",
       "112                    0.0           5.845808              1   \n",
       "113                    0.0           6.527432              1   \n",
       "\n",
       "     daily_sentiment_mean_3  daily_sentiment_std_3  daily_sentiment_mean_7  \\\n",
       "0                  0.846911               1.070261                0.846911   \n",
       "1                  0.846911               1.070261                0.846911   \n",
       "2                  0.090122               1.070261                0.090122   \n",
       "3                  0.349622               0.880199                0.349622   \n",
       "4                  0.067319               0.769856                0.262217   \n",
       "..                      ...                    ...                     ...   \n",
       "109               -0.033357               0.016115                0.368044   \n",
       "110                0.144428               0.308145                0.311235   \n",
       "111                0.455916               0.457432                0.421193   \n",
       "112                0.694855               0.275567                0.400608   \n",
       "113                0.890447               0.001042                0.442836   \n",
       "\n",
       "     daily_sentiment_std_7  daily_sentiment_mean_14 daily_sentiment_std_14  \\\n",
       "0                 1.070261                 0.846911               1.070261   \n",
       "1                 1.070261                 0.846911               1.070261   \n",
       "2                 1.070261                 0.090122               1.070261   \n",
       "3                 0.880199                 0.349622               0.880199   \n",
       "4                 0.739635                 0.262217               0.739635   \n",
       "..                     ...                      ...                    ...   \n",
       "109               0.370627                 0.468990               0.300142   \n",
       "110               0.303751                 0.450591               0.289895   \n",
       "111               0.378631                 0.482944               0.316531   \n",
       "112               0.419552                 0.481821               0.331955   \n",
       "113               0.463077                 0.489599               0.341175   \n",
       "\n",
       "    has_spike  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "..        ...  \n",
       "109         0  \n",
       "110         0  \n",
       "111         0  \n",
       "112         0  \n",
       "113         0  \n",
       "\n",
       "[114 rows x 35 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eaaeef2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged shape: (791, 27)\n",
      "Date range  : 2022-07-21 00:00:00 → 2025-09-15 00:00:00\n",
      "Saved to    : /Users/hwang-yejin/Desktop/Financial Time Series Forecasting with Deep Learning Models and Social Media Sentiment/data/processed/tsla_price_sentiment_spike_merged_20220721_20250915.csv\n",
      "NaN counts (tail):\n",
      "spike_loess_upper           570\n",
      "spike_spike_intensity       570\n",
      "spike_spike_presence        570\n",
      "spike_smoothed              570\n",
      "spike_post_count            570\n",
      "spike_impact_trading_day    570\n",
      "daily_sentiment             559\n",
      "rolling_volatility            0\n",
      "date                          0\n",
      "unique_id                     0\n",
      "time_idx                      0\n",
      "cumulative_return             0\n",
      "return_1d                     0\n",
      "Date                          0\n",
      "Open                          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 5) Save\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "start, end = merged['date'].min().strftime('%Y%m%d'), merged['date'].max().strftime('%Y%m%d')\n",
    "out_path = OUT_DIR / f'tsla_price_sentiment_spike_merged_{start}_{end}.csv'\n",
    "df.to_csv(out_path, index=False)\n",
    "\n",
    "print('Merged shape:', merged.shape)\n",
    "print('Date range  :', merged[\"date\"].min(), '→', merged[\"date\"].max())\n",
    "print('Saved to    :', out_path)\n",
    "print('NaN counts (tail):')\n",
    "print(merged.isna().sum().sort_values(ascending=False).head(15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
